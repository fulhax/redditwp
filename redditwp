#!/bin/env python3
import os
import re
import pwd
import sys
import json
import argparse
import logging as log

from os import path
from copy import copy
from time import sleep
from pathlib import Path

import certifi
import urllib3

from PIL import Image

http = urllib3.PoolManager(
    cert_reqs='CERT_REQUIRED',
    ca_certs=certifi.where()
)

cachefile = '/tmp/redditwp.cache'
options = []
cache = []

if not sys.platform.startswith("win") and sys.stderr.isatty():
    def add_color_emit_ansi(fn):
        """Add methods we need to the class."""
        def new(*args):
            """Method overload."""
            if len(args) == 2:
                new_args = (args[0], copy(args[1]))
            else:
                new_args = (args[0], copy(args[1]), args[2:])
            if hasattr(args[0], 'baseFilename'):
                return fn(*args)
            levelno = new_args[1].levelno
            if levelno >= 50:
                color = '\x1b[31;5;7m\n '  # blinking red with black
            elif levelno >= 40:
                color = '\x1b[31m'  # red
            elif levelno >= 30:
                color = '\x1b[33m'  # yellow
            elif levelno >= 20:
                color = '\x1b[32m'  # green
            elif levelno >= 10:
                color = '\x1b[35m'  # pink
            else:
                color = '\x1b[0m'  # normal
            try:
                new_args[1].msg = color + str(new_args[1].msg) + ' \x1b[0m'
            except Exception as reason:
                print(reason)  # Do not use log here.
            return fn(*new_args)
        return new
    # all non-Windows platforms support ANSI Colors so we use them
    log.StreamHandler.emit = add_color_emit_ansi(log.StreamHandler.emit)


def parse_args():
    username = pwd.getpwuid(os.getuid()).pw_name

    parser = argparse.ArgumentParser(
        description='Get wallpapers from your choice of subreddits!',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument(
        'subreddits',
        type=str,
        nargs='*',
        help='Your choice of subreddits where to download Images'
    )

    parser.add_argument(
        '--min-score',
        type=int,
        default=500,
        help='Minimum score to download'
    )
    parser.add_argument(
        '--debuglevel',
        type=int,
        default=log.INFO,
        help='Set debug level'
    )
    parser.add_argument(
        '--directory',
        type=str,
        default="/home/" + username + "/images/wallpapers",
        help='Download directory'
    )
    parser.add_argument(
        '--list',
        type=str,
        choices=['hot', 'top', 'new', 'random', 'rising'],
        default="hot",
        help='Subreddit list'
    )
    parser.add_argument(
        '--time',
        type=str,
        choices=['hour', 'day', 'week', 'month', 'year', 'all'],
        default="all",
        help='Subreddit time (dont work with new or hot)'
    )
    parser.add_argument(
        '--res',
        type=str,
        default='1920x1080',
        help='Minimum resolution to save (used with clean)'
    )
    parser.add_argument(
        '--aspect',
        type=str,
        default='16:9',
        help='Aspect ratio (used with clean / clean-by-aspect)'
    )
    parser.add_argument(
        '--count',
        type=int,
        default=25,
        help='Number of threads to check'
    )
    parser.add_argument(
        '-subfolders',
        default=False,
        action='store_true',
        help='Store in subfolders with subredditname'
    )
    parser.add_argument(
        '-no-clean',
        default=False,
        action='store_true',
        help='Remove small images'
    )
    parser.add_argument(
        '-recursive-clean',
        default=False,
        action='store_true',
        help='Clean folders recursive'
    )
    parser.add_argument(
        '-force-clean',
        default=False,
        action='store_true',
        help="Don't prompt before cleaning folders"
    )
    parser.add_argument(
        '-resize-larger',
        default=False,
        action='store_true',
        help='Resize images larger than the specified minimum resolution'
    )
    parser.add_argument(
        '-clean-by-aspect',
        default=False,
        action='store_true',
        help='Removed images with incorrect aspect'
    )
    parser.add_argument(
        '-no-download',
        default=False,
        action='store_true',
        help='If you just want to run clean'
    )
    parser.add_argument(
        '-nsfw',
        default=False,
        action='store_true',
        help='Download nsfw'
    )
    parser.add_argument(
        '-ignore-cache',
        default=False,
        action='store_true',
        help='Ignore previously cached urls'
    )

    if len(sys.argv) == 1:
        parser.print_help()
        sys.exit(0)

    args = parser.parse_args()
    return args


def gcd(a, b):
    if (b == 0):
        return a

    return gcd(b, a % b)


def cleanbyaspect(im, iw, ih, fpath):
    r = gcd(iw, ih)
    aspect = "%d:%d" % (int(iw/r), int(ih/r))

    if aspect != options.aspect:
        os.system("rm '" + fpath + "'")

        log.info("Removed %s it had incorrect aspect (%s)" % (
            fpath,
            aspect
        ))

        return True


def cleanbysize(im, iw, ih, fpath):
    width, height = im.size

    if options.clean_by_aspect is True:
        if cleanbyaspect(im, width, height, fpath) is True:
            return

    if width < iw or height < ih:
        os.system("rm '" + fpath + "'")

        log.info("Removed %s it was lowres (%dx%d)" % (
            fpath,
            width,
            height
        ))
    elif width > iw or height > ih:
        if options.resize_larger is True:
            try:
                im.thumbnail((iw, ih))
                im.save(fpath)

                log.info("Resized %s it was highres (%dx%d)" % (
                    fpath,
                    width,
                    height
                ))
            except Exception as e:
                log.error(e)


def cleanunwanted(dir):
    if options.force_clean is False:
        log.warning(
            "Are you sure you want to clean %s [y/N]?\n"
            "(this will run removal check for all images in the"
            " folder not just the currently downloaded)"
            % path.realpath(dir)
        )

        yes = {'yes', 'y'}
        no = {'no', 'n', ''}

        while True:
            choice = input()

            if choice in yes:
                break
            elif choice in no:
                return
            else:
                print("Please respond with 'yes' or 'no' (or 'y' or 'n').")

    (hqw, hqh) = options.res.split('x')

    iw = int(hqw)
    ih = int(hqh)

    log.info("Cleaning %s" % path.realpath(dir))
    for fpath in returnwallpaper(dir):
        with Image.open(fpath) as im:
            cleanbysize(im, iw, ih, fpath)


def returnwallpaper(dir):
    if options.recursive_clean is False:
        return [
            path.join(dir, f) for f in os.listdir(dir) if re.search(
                r'.*(jpg|png)$',
                f,
                re.IGNORECASE
            )
        ]

    output = []

    for root, dirs, files in os.walk(dir):
        output += [
            path.join(root, f) for f in files if re.search(
                r'.*(jpg|png)$',
                f,
                re.IGNORECASE
            )
        ]

    return output


def downloadimage(domain, imurl, dir):
    if imurl in cache:
        log.debug("Skipping %s, it exists in url-cache" % (imurl))
        return 0

    cache.append(imurl)

    try:
        imgdata = http.request(
            'HEAD',
            imurl
        )

        content_type = imgdata.headers['content-type']

        if not re.search(r'image\/(jpeg|png)', content_type):
            log.debug("Invalid content type %s" % content_type)
            return 0

        try:
            filename = imgdata.info()['Content-Disposition'].lower()
            filename = re.sub(r'.*filename="([^"]+)".*', r'\1', filename)
        except KeyError:
            filename = os.path.basename(
                urllib3.util.url.parse_url(imurl).path
            ).lower()

        if not re.search(r'\.(jpg|png)$', filename):
            filename = re.sub(r'(jpg|png).*', r'\1', filename)

            if not re.search(r'\.(jpg|png)$', filename):
                log.debug("Invalid filename %s" % filename)
                return 0

        if Path(dir + "/" + filename).exists():
            log.warning("%s already exists in %s" % (filename, dir))
            return 0

        imgdata = http.request(
            'GET',
            imurl,
            preload_content=False
        )

        log.info("Downloading %s as %s" % (imurl, content_type))

        with open((dir + "/" + filename), "wb") as out:
            while True:
                data = imgdata.read(65536)
                if not data:
                    break
                out.write(data)

        imgdata.release_conn()

        return 1

    except Exception as e:
        log.error(e)
        pass


def download(url, subreddit, dir):
    try:
        obj = http.request('GET', url)
        data = json.loads(obj.data.decode('utf-8'))
    except Exception as e:
        log.error(e)
        return

    if 'error' in data:
        log.error("%d: %s" % (data['error'], data['message']))
        return

    count = 0

    if not path.exists(dir):
        log.debug("Creating folder %s" % path.realpath(dir))
        os.makedirs(dir)

    for i in data["data"]["children"]:
        domain = i["data"]["domain"]
        imurl = i["data"]["url"]
        nsfw = i["data"]["over_18"]
        score = i["data"]["score"]

        if score <= options.min_score:
            log.debug("Skipping %s, score %d is less than %d" % (
                imurl,
                score,
                options.min_score
            ))
            continue

        if nsfw and not options.nsfw:
            log.debug("Skipping %s because it's flagged as nsfw!" % imurl)
            continue

        count += downloadimage(domain, imurl, dir)

    log.info("Downloaded %d images" % (count))

    return data["data"]["after"]


def writecache():
    global cache
    mode = 'w+'

    if options.ignore_cache:
        mode = 'w'

    with open(cachefile, mode) as f:
        for item in cache:
            f.write("%s\n" % item)
        f.close()


def readcache():
    global cache

    if os.path.exists(cachefile) and not options.ignore_cache:
        with open(cachefile, "r") as f:
            cache = f.read().split('\n')
            f.close()


if __name__ == '__main__':
    options = parse_args()

    log.basicConfig(
        level=options.debuglevel,
        format="%(message)s",
    )

    url = 'https://www.reddit.com/r/%s/%s.json?after=%s&t=%s&limit=%d'

    if options.no_download is False:
        readcache()

        for subreddit in options.subreddits:
            count = 0
            limit = min(options.count, 100)
            step = limit
            after = ''

            dir = options.directory
            if options.subfolders:
                dir += '/' + subreddit

            while True:
                log.info("Checking %s subreddit %s (%d - %d)" % (
                    options.list,
                    subreddit,
                    count,
                    step
                ))

                after = download(
                    url % (
                        subreddit,
                        options.list,
                        after,
                        options.time,
                        (step - count)
                    ),
                    subreddit,
                    dir
                )

                count += limit
                step += limit

                if (step > options.count):
                    step = options.count % step

                sleep(2)

                if count >= options.count:
                    break

            if options.no_clean is False:
                cleanunwanted(dir)

        writecache()
    elif options.no_clean is False:
        cleanunwanted(options.directory)
